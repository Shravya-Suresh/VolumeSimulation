{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xSTjjM71lgR",
        "outputId": "c263082e-31ab-4f5c-a37e-c16e43483d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# ZIPs from MyDrive:\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# List of zip files\n",
        "zip_files = ['speech.zip', 'sing.zip', 'noise.zip']\n",
        "\n",
        "# Extract each zip into the same folder\n",
        "for zip_file in zip_files:\n",
        "    zip_path = os.path.join(drive_path, zip_file)\n",
        "    if os.path.exists(zip_path):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "    else:\n",
        "        print(f\"File not found: {zip_path}\")\n"
      ],
      "metadata": {
        "id": "C-2OkFDf1oLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labeled_data(data_folder):\n",
        "    label_map = {'sing': 0, 'speech': 1, 'noise': 2}\n",
        "    data = []\n",
        "    for class_name in os.listdir(data_folder):\n",
        "        class_path = os.path.join(data_folder, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        label = label_map.get(class_name)\n",
        "        if label is None:\n",
        "            continue\n",
        "        for filename in os.listdir(class_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                filepath = os.path.join(class_path, filename)\n",
        "                # Preprocess and convert to spectrogram immediately\n",
        "                mel_spec = preprocessing(filepath)\n",
        "                data.append((mel_spec, label))\n",
        "    return data"
      ],
      "metadata": {
        "id": "qAkdVGJX1oIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "import torch.nn\n",
        "from torchaudio import transforms as T"
      ],
      "metadata": {
        "id": "BY0tgp0z1oFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_waveforms_and_standard_waveform (filepath, sample_rate = 44100):\n",
        "  waveform, org_samplerate = torchaudio.load(filepath)\n",
        "\n",
        "  if org_samplerate != sample_rate:\n",
        "    resampler = T.Resample(orig_freq = org_samplerate, new_freq=44100)\n",
        "    waveform = resampler(waveform)\n",
        "\n",
        "  return waveform, sample_rate\n"
      ],
      "metadata": {
        "id": "-ArXkGAz1oCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_channel(waveform):\n",
        "  if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "  return waveform"
      ],
      "metadata": {
        "id": "3uDcUTUv1n_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_melspectogram(waveform, sample_rate = 44100):\n",
        "  mel_transform = T.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=64)\n",
        "  mel_spec = mel_transform(waveform)\n",
        "  return mel_spec"
      ],
      "metadata": {
        "id": "JT1v-rUo1n8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_or_pad (waveform, max_duraction = 5, sample_rate = 44100):\n",
        "  max_len = max_duraction * sample_rate\n",
        "  if waveform.shape[1] > max_len:\n",
        "    waveform = waveform[:, :max_len]\n",
        "  else:\n",
        "    padding = max_len - waveform.shape[1]\n",
        "    waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "  return waveform, sample_rate"
      ],
      "metadata": {
        "id": "KxoRAAGi1n5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing (data, sample_rate = 44100, duration = 5):\n",
        "  waveform, sample_rate = create_waveforms_and_standard_waveform(data, sample_rate)\n",
        "  waveform = create_single_channel(waveform)\n",
        "  waveform, sample_rate = trim_or_pad(waveform, duration, sample_rate)\n",
        "  mel_spec = create_melspectogram(waveform, sample_rate)\n",
        "  mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6)\n",
        "  return mel_spec"
      ],
      "metadata": {
        "id": "NwJRw5Fc1n2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, data, augment=False):\n",
        "        self.data = data\n",
        "        self.augment = augment\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel_spec, label = self.data[idx]\n",
        "\n",
        "        # Ensure proper shape and type\n",
        "        mel_spec = mel_spec.clone().detach()\n",
        "        if len(mel_spec.shape) == 2:\n",
        "            mel_spec = mel_spec.unsqueeze(0)\n",
        "\n",
        "        # Convert to float32 if needed\n",
        "        mel_spec = mel_spec.float()\n",
        "\n",
        "        if self.augment:\n",
        "            # Time masking (axis=2 is time)\n",
        "            if random.random() > 0.5:\n",
        "                time_mask_param = random.randint(10, 20)\n",
        "                mel_spec = torchaudio.functional.mask_along_axis(\n",
        "                    mel_spec, time_mask_param, mask_value=0, axis=2)\n",
        "\n",
        "            # Frequency masking (axis=1 is frequency)\n",
        "            if random.random() > 0.5:\n",
        "                freq_mask_param = random.randint(5, 10)\n",
        "                mel_spec = torchaudio.functional.mask_along_axis(\n",
        "                    mel_spec, freq_mask_param, mask_value=0, axis=1)\n",
        "\n",
        "            # Add noise\n",
        "            if random.random() > 0.5:\n",
        "                noise = torch.randn_like(mel_spec) * 0.01\n",
        "                mel_spec = mel_spec + noise\n",
        "\n",
        "        return mel_spec, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "fJIalqsT1nzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model\n",
        "class AudioCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AudioCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ELU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "s2TImiQv1nvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create labeled data with spectrograms\n",
        "data = create_labeled_data('/content/dataset')\n",
        "dataset = AudioDataset(data)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 2. Split into train/val\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, stratify=[label for _, label in data])\n",
        "\n",
        "# 3. Create datasets\n",
        "train_dataset = AudioDataset(train_data, augment=True)\n",
        "val_dataset = AudioDataset(val_data, augment=False)\n",
        "\n",
        "# 4. Create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model setup\n",
        "model = AudioCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.0001,\n",
        "    weight_decay=1e-5\n",
        ")"
      ],
      "metadata": {
        "id": "J584USy73R1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create save directory with timestamp\n",
        "save_dir = f'saved_models_new{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training configuration\n",
        "best_val_acc = 0\n",
        "patience = 5\n",
        "trigger_times = 0\n",
        "early_stop = False\n",
        "\n",
        "for epoch in range(20):\n",
        "    if early_stop:\n",
        "        break\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for mel_spec, label in train_loader:\n",
        "        if len(mel_spec.shape) == 3:\n",
        "            mel_spec = mel_spec.unsqueeze(1)\n",
        "\n",
        "        outputs = model(mel_spec)\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for mel_spec, label in val_loader:\n",
        "            if len(mel_spec.shape) == 3:\n",
        "                mel_spec = mel_spec.unsqueeze(1)\n",
        "            outputs = model(mel_spec)\n",
        "            loss = criterion(outputs, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_acc = 100 * correct / total\n",
        "    avg_train_loss = train_loss/len(train_loader)\n",
        "    avg_val_loss = val_loss/len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}, \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "          f\"Val Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Early stopping and model saving logic\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        trigger_times = 0  # Reset patience counter\n",
        "\n",
        "        # Save best model versions\n",
        "        torch.save(model.state_dict(), f'{save_dir}/best_model_weights.pth')\n",
        "        torch.save(model, f'{save_dir}/best_full_model.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_accuracy': val_acc,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'train_loss': avg_train_loss,\n",
        "        }, f'{save_dir}/best_checkpoint.pth')\n",
        "\n",
        "        print(f\"↳ New best model saved (acc: {val_acc:.2f}%)\")\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"↳ No improvement ({trigger_times}/{patience})\")\n",
        "        if trigger_times >= patience:\n",
        "            print(\"!!! Early stopping triggered !!!\")\n",
        "            early_stop = True\n",
        "\n",
        "# Final save regardless of early stopping\n",
        "torch.save({\n",
        "    'final_model_state_dict': model.state_dict(),\n",
        "    'final_accuracy': best_val_acc,\n",
        "    'final_epoch': epoch,\n",
        "    'early_stopped': early_stop,\n",
        "}, f'{save_dir}/final_model.pth')\n",
        "\n",
        "# Training summary\n",
        "print(\"\\n=== Training Complete ===\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Stopped at epoch {epoch+1} {'(early stopped)' if early_stop else ''}\")\n",
        "print(f\"\\nSaved models in '{save_dir}':\")\n",
        "print(f\"- best_model_weights.pth (state dict)\")\n",
        "print(f\"- best_full_model.pth (complete model)\")\n",
        "print(f\"- best_checkpoint.pth (full training state)\")\n",
        "print(f\"- final_model.pth (final trained model)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXQ62tiC1noD",
        "outputId": "c3c4fca3-73de-4f76-ec4a-95741cb1b98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01, Train Loss: 1.0835, Val Loss: 0.9023, Val Accuracy: 60.08%\n",
            "↳ New best model saved (acc: 60.08%)\n",
            "Epoch 02, Train Loss: 0.8721, Val Loss: 0.6213, Val Accuracy: 79.47%\n",
            "↳ New best model saved (acc: 79.47%)\n",
            "Epoch 03, Train Loss: 0.7592, Val Loss: 0.5593, Val Accuracy: 78.71%\n",
            "↳ No improvement (1/5)\n",
            "Epoch 04, Train Loss: 0.7153, Val Loss: 0.5666, Val Accuracy: 74.14%\n",
            "↳ No improvement (2/5)\n",
            "Epoch 05, Train Loss: 0.6486, Val Loss: 0.4488, Val Accuracy: 88.21%\n",
            "↳ New best model saved (acc: 88.21%)\n",
            "Epoch 06, Train Loss: 0.5948, Val Loss: 0.4530, Val Accuracy: 85.93%\n",
            "↳ No improvement (1/5)\n",
            "Epoch 07, Train Loss: 0.5843, Val Loss: 0.3532, Val Accuracy: 90.87%\n",
            "↳ New best model saved (acc: 90.87%)\n",
            "Epoch 08, Train Loss: 0.5728, Val Loss: 0.4082, Val Accuracy: 87.83%\n",
            "↳ No improvement (1/5)\n",
            "Epoch 09, Train Loss: 0.5235, Val Loss: 0.3500, Val Accuracy: 90.11%\n",
            "↳ No improvement (2/5)\n",
            "Epoch 10, Train Loss: 0.4991, Val Loss: 0.3163, Val Accuracy: 91.25%\n",
            "↳ New best model saved (acc: 91.25%)\n",
            "Epoch 11, Train Loss: 0.4738, Val Loss: 0.3277, Val Accuracy: 90.49%\n",
            "↳ No improvement (1/5)\n",
            "Epoch 12, Train Loss: 0.4634, Val Loss: 0.3164, Val Accuracy: 91.63%\n",
            "↳ New best model saved (acc: 91.63%)\n",
            "Epoch 13, Train Loss: 0.4568, Val Loss: 0.2849, Val Accuracy: 91.25%\n",
            "↳ No improvement (1/5)\n",
            "Epoch 14, Train Loss: 0.4662, Val Loss: 0.3099, Val Accuracy: 91.25%\n",
            "↳ No improvement (2/5)\n",
            "Epoch 15, Train Loss: 0.4320, Val Loss: 0.2942, Val Accuracy: 92.02%\n",
            "↳ New best model saved (acc: 92.02%)\n",
            "Epoch 16, Train Loss: 0.4142, Val Loss: 0.2597, Val Accuracy: 93.16%\n",
            "↳ New best model saved (acc: 93.16%)\n",
            "Epoch 17, Train Loss: 0.4287, Val Loss: 0.2588, Val Accuracy: 92.78%\n",
            "↳ No improvement (1/5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsCWJZjU49Mj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}